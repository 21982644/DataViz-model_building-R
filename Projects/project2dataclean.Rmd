---
title: "CITS4009-Project 2"
output: 
    html_document
---
#### Xiaoyu (21982644)

## Introduction
The dataset used for this project is pretty much the same as for the dataset used for project 1 (EDA) except this dataset has more observations. This dataset contains collected information on the mining injuries at U.S from 2000 to 2015 with 57 features and over 200,000 observations.

### Load Libraies
```{r}
library(dplyr)
library(vtreat)
library(ROCR)
library(ROCit)
library(rpart)
library(ggplot2)
library(knitr)
library(xgboost)
library(reshape2)
```

### Load the Data
```{r}
setwd("~/Downloads/CITS4009/Projects")
main <- read.csv("us_data.csv")
```

### Data Pre-processing
The data are cleaned as per project 1 with some modifications. Since we are already familiar with the data, this cleaning steps and explanation are not shown in full details. 

"DAYS_LOST","DAYS_RESTRICTED", "SCHEDULE_CHARGE" contain information about the degree of injury and, we'll get very good f1 score(0.988), recall(1), precision(0.977) for decision tree model. However, I don't thinks it's appropriate to include those variables in the model, it's more like telling the result rather than predict. 

Variable OCCUPATION will also be excluded from here, reasons for this is it contains too many levels (more than 180 categories) and even though the training set contains approximate 80% of the data, there were still some categories that in the calibration/test set but not in training set, which makes it hard to build the model. There are ways to solve this problem such as orders the levels of the OCCUPATION by the number of occurrence of each level in one class. However, in this project we'll exclude the variable.
```{r}
# select columns
keep <- c("SUBUNIT","ACCIDENT_DT","CAL_YR","CAL_QTR","ACCIDENT_TIME", "DEGREE_INJURY_CD", "FIPS_STATE_CD", "UG_LOCATION", "UG_MINING_METHOD", "MINING_EQUIP", "SHIFT_BEGIN_TIME", "CLASSIFICATION", "ACCIDENT_TYPE", "NO_INJURIES", "TOT_EXPER", "MINE_EXPER","JOB_EXPER", "ACTIVITY","INJURY_SOURCE","NATURE_INJURY", "INJ_BODY_PART", "TRANS_TERM", "RETURN_TO_WORK_DT", "IMMED_NOTIFY","COAL_METAL_IND")

injury <- main[which(names(main) %in% keep)]
```

```{r}
# change invalid value to NA and fill the NA
injury <- mutate(injury, ACCIDENT_TIME = ifelse(ACCIDENT_TIME > 2400, NA, ACCIDENT_TIME)) %>% mutate(injury,SHIFT_BEGIN_TIME= ifelse(SHIFT_BEGIN_TIME > 2400, NA, SHIFT_BEGIN_TIME))

# change the time in a range of 1-24
injury <- injury %>% mutate(ACCIDENT_TIME_NEW=case_when(
  ACCIDENT_TIME < 100  ~  floor(injury$ACCIDENT_TIME/10),
  ACCIDENT_TIME >= 100 ~ floor(injury$ACCIDENT_TIME/100))) %>% 
  mutate(SHIFT_BEGIN_TIME_NEW=case_when(
  SHIFT_BEGIN_TIME < 100  ~  floor(injury$SHIFT_BEGIN_TIME /10),
  SHIFT_BEGIN_TIME >= 100 ~ floor(injury$SHIFT_BEGIN_TIME /100)))

# fill the NA as the median based the ACCIDENT_TIME and SHIFT_BEGIN_TIME, from EDA process we know that those two values are higher related 

injury <- injury %>%
    group_by(SHIFT_BEGIN_TIME_NEW) %>%
    mutate(Amedian = median(ACCIDENT_TIME_NEW, na.rm=TRUE))  %>% group_by(ACCIDENT_TIME_NEW) %>%
    mutate(Smedian = median(SHIFT_BEGIN_TIME_NEW, na.rm=TRUE))

injury <- injury %>% mutate(SHIFT_BEGIN_TIME_NEW = ifelse(is.na(SHIFT_BEGIN_TIME_NEW),Smedian, SHIFT_BEGIN_TIME_NEW)) %>% mutate(ACCIDENT_TIME_NEW = ifelse(is.na(ACCIDENT_TIME_NEW),Amedian,ACCIDENT_TIME_NEW))
```

```{r}
# add a column as days return to work 
injury <- within(injury, {
     ACCIDENT_DT      <- as.POSIXct(ACCIDENT_DT, format='%d/%m/%Y')
     RETURN_TO_WORK_DT<- as.POSIXct(RETURN_TO_WORK_DT, format='%m/%d/%Y')
})

injury$DAY_AWY <- difftime(injury$RETURN_TO_WORK_DT,injury$ACCIDENT_DT, units="days")
injury$DAY_AWY <- as.numeric(injury$DAY_AWY)
```

```{r}
# fill NA for experience variables 
injury <- mutate(injury, TOT_EXPER_NEW = ifelse(is.na(TOT_EXPER),
                                                       JOB_EXPER,
                                                       TOT_EXPER))

injury <- mutate(injury, TOT_EXPER_NEW = ifelse(is.na(TOT_EXPER),
                                                       MINE_EXPER,
                                                       TOT_EXPER))

injury <- mutate(injury, JOB_EXPER_NEW = ifelse(is.na(JOB_EXPER),
                                                       TOT_EXPER,
                                                       JOB_EXPER))

injury <- mutate(injury, MINE_EXPER_NEW = ifelse(is.na(MINE_EXPER),
                                                       TOT_EXPER,
                                                       MINE_EXPER))
```

```{r}
# still have some NAs, use vtreat to prepare the data

 varlist <- setdiff(colnames(injury),"DEGREE_INJURY_CD")

 treatment_plan <- design_missingness_treatment(injury, varlist = varlist)

 training_prepared <- prepare(treatment_plan,injury)
```

```{r}
# remove injury df to free memory
rm(injury)
```

## Part 1 - Classification

### Select the target variable
The response variable chosen for this project is the degree of injury of the accident, no injured or injured.

I've focused on this target variable to help to classify whether the accidents will be if employee is injured it will cause days lost from work/days restricted etc. In this case, the company can plan ahead and take countermeasures.The labels assigned to this variable are no for not injured or yes for workers is injured because the accident. The aim of this project is to use a number of feature variables to create a classification model to assess if an incident will involve extra attention or if countermeasures are needed due to injury.

Create a target variable based on the the degree injury number. 0 and 6 indicate there that the employee had no injuries.
```{r}
training_prepared <-  training_prepared %>% 
                      subset(DEGREE_INJURY_CD %in% c("0", "1","2","3","4","5","6"))  %>%
                      mutate(DEGREE_INJURY_PRED = ifelse(DEGREE_INJURY_CD %in% c("0","6"), "no","yes"))%>%
                      mutate(DEGREE_INJURY_PRED = as.factor(DEGREE_INJURY_PRED))

table(training_prepared$DEGREE_INJURY_PRED)
```

The split between day lost and no day lost is not 1:1 balanced but are very close, so addition methods like under-sampling or over-sampling is not needed.
```{r}
round(prop.table(table(training_prepared$DEGREE_INJURY_PRED)), 2)
```

Convert indicator variables to factor.
```{r message=FALSE, warning=FALSE}
indicator <- names(training_prepared)[grepl("isBAD",names(training_prepared))]

for(d in indicator) {
      training_prepared[, d] <- as.factor(training_prepared[, d])
}
```

### Select feature variables
Select input variables that are likely to be most useful to a model in order to predict the target variable, in this case, we want to select variables from the dataset that could help us to predict whether the incidents will cause day lost to the workers. The information gathered from EDA process and the data dictionary that was given are used to make an informed decision about the variable we could use. And this have almost done in the data cleaning step, which some variables were selected and kept in the previous step. However, we still need to filter some variables that created in previous step. 

```{r}
training_prepared <- training_prepared[,-which(names(training_prepared) %in% c("Amedian", "Smedian","ACCIDENT_DT","RETURN_TO_WORK_DT"))]
```

### Splitting
The code below is adapted from the lecture slide. 90% of the data will be used for training and the rest of them will be used for training. Out of the 90% of the training set, 10% of them will be used for calibration.
```{r}
set.seed(12345678)
rgroup <- runif(nrow(training_prepared)) < 0.9
dTrainAll <- training_prepared[rgroup,]
dTest <- training_prepared[!rgroup,]

outcomes <- c('DEGREE_INJURY_PRED','DEGREE_INJURY_CD','DAY_AWY')

vars <- setdiff(colnames(dTrainAll), outcomes)
catVars <- vars[sapply(dTrainAll[, vars], class) %in% c('factor', 'character')]
numericVars <- vars[sapply(dTrainAll[, vars], class) %in% c('numeric', 'integer')] 

useForCal <- rbinom(n=dim(dTrainAll)[1], size=1, prob=0.1) > 0 
dCal <- subset(dTrainAll, useForCal)
dTrain <- subset(dTrainAll, !useForCal)

rm(list=c('training_prepared','dTrainAll'))
```

### Single Variable Model(SVM)
#### Categorical
Function for SVM predictions for categorical variables.
```{r}
pos <- "yes"
mkPredC <- function(outCol, varCol, appCol) {
  pPos <- sum(outCol == pos) / length(outCol)
  naTab <- table(as.factor(outCol[is.na(varCol)]))
  pPosWna <- (naTab/sum(naTab))[pos]
  vTab <- table(as.factor(outCol), varCol)
  pPosWv <- (vTab[pos, ] + 1.0e-3*pPos) / (colSums(vTab) + 1.0e-3) 
  pred <- pPosWv[appCol]
  pred[is.na(appCol)] <- pPosWna
  pred[is.na(pred)] <- pPos
  pred
}
```

Data type has change to multi-type, need to change back to data.frame only.
```{r}
dTrain <- as.data.frame(dTrain)
dCal <- as.data.frame(dCal)
dTest <- as.data.frame(dTest)
```

```{r}
outcome <- "DEGREE_INJURY_PRED"
for(v in catVars) {
  pi <- paste('pred',v,sep='')
  dTrain[,pi] <- mkPredC(dTrain[,outcome], dTrain[,v], dTrain[,v])
  dCal[,pi] <- mkPredC(dTrain[,outcome], dTrain[,v], dCal[,v])
  dTest[,pi] <- mkPredC(dTrain[,outcome], dTrain[,v], dTest[,v])
}
```

Evaluate SVM for categorical variables.
```{r}
calcAUC <- function(predcol,outcol) {
  perf <- ROCR::performance(prediction(predcol,outcol==pos),'auc')
  as.numeric(perf@y.values) 
}
```

Processing all categorical variables, print out the area under curve (AUC) only it is greater or equal to 0.5. 
```{r}
for(v in catVars) {
  pi <- paste('pred', v, sep='')
  aucTrain <- calcAUC(dTrain[,pi], dTrain[,outcome]) 
  if (aucTrain >= 0.5) {
    aucCal <- calcAUC(dCal[,pi], dCal[,outcome]) 
    print(sprintf(
      "%s: trainAUC: %4.3f; calibrationAUC: %4.3f",
      pi, aucTrain, aucCal))
  }
}
```

From the output, we can see there are a few very impressing variables with high AUC such as injury body part, accident type and nature injury with AUC score over 0.8. 

#### Filter out indicators.
```{r}
catVars <- setdiff(catVars,indicator)
```

#### Numeric 
Function for SVM predictions for numeric variables.
```{r}
mkPredN <- function(outCol,varCol,appCol) {
  cuts <- unique(as.numeric(quantile(varCol, probs=seq(0, 1, 0.1), na.rm=T)))
  varC <- cut(varCol, cuts)
  appC <- cut(appCol, cuts)
  mkPredC(outCol, varC, appC)
}
```

Processing all numeric variables, print out the area under curve (AUC) only it is greater or equal to 0.5. 
```{r}
for(v in numericVars) {
  pi<-paste('pred',v,sep='')
  dTrain[,pi] <- mkPredN(dTrain[,outcome], dTrain[,v], dTrain[,v])
  dTest[,pi] <- mkPredN(dTrain[,outcome], dTrain[,v], dTest[,v])
  dCal[,pi] <- mkPredN(dTrain[,outcome], dTrain[,v], dCal[,v])
  aucTrain <- calcAUC(dTrain[,pi],dTrain[,outcome])
  
  if(aucTrain>=0.5) {
    aucCal<-calcAUC(dCal[,pi],dCal[,outcome])
    print(sprintf(
      "%s, trainAUC: %4.3f calibrationAUC: %4.3f",
      pi,aucTrain,aucCal))
  }
}
```
Variable DAY_AWY with AUC of 0.966, which is not shown here (same reason as days lost and days away variable). The best single numeric variable model is the Total experience variabe with calibration AUC score of 0.665.

#### Calculate loglikelihood for feature selection
```{r}
# define function that calculate log likelihood
logLikelihood <- function(outCol, predCol, posl=pos) {
  sum(ifelse(outCol==pos, log(predCol), log(1-predCol)))
}
```

Log null
```{r}
baseRateCheck <- logLikelihood(dCal[,outcome], sum(dCal[,outcome]==pos)/length(dCal[,outcome]) )
```

Select categorical variables only if the log likelihood is over 500. 
```{r}
selPredVars <- c()
selVars <- c()
minStep <- 500

for(v in catVars) {
  pi <- paste('pred',v,sep='')
  liCheck <- 2*((logLikelihood(dCal[,outcome],dCal[,pi])
                 - baseRateCheck))
  if(liCheck>minStep) {
    print(sprintf("%s, calibrationScore: %g",pi,liCheck))
    selPredVars <- c(selPredVars,pi)
    selVars <- c(selVars, v)
  }
}
```

Select numeric variables only if the log likelihood is over 500. 
```{r}
for(v in numericVars) {
    pred <- paste(outcome, 'pred', v, sep='_')
    liCheck <- 2*((logLikelihood(dCal[,outcome], dCal[, pi]) - baseRateCheck) - 1)
    if(liCheck >= minStep) {
        print(sprintf("%s, calibrationScore: %g", v, liCheck))
        selPredVars <- c(selPredVars,pi)
        selVars     <- c(selVars, v)
    }
}
```

```{r}
selVars
```

### Multivariate models

Data frame that store the model and performance measure for models.
```{r}
modelMeasure <- data.frame(modelname=character(), precision=double(), recall = double(), f1 = double(), accuracy = double(), trainAUC=double(), calAUC=double(), logLikelihood=double())
```

#### Model evaluation
Functions that plot graphs 
```{r}
# plot ROC curve
 plotROC <- function(pf,modelname="mn",titleString="ROC plot") { 
   ggplot() + 
     geom_line(data=pf, aes(x=FalsePositiveRate, y=TruePositiveRate), colour="red") +
     labs(title=paste(modelname, titleString)) +
     geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
     theme_bw()
 }

# distribution plot
distribution <- function(prediction, calTrue,modelname="mn"){
  ggplot(data.frame(predictions=prediction, calTrue = calTrue),
          aes(x=predictions, color=calTrue, linetype=calTrue)) +
          geom_density()  + 
          labs(title = paste(modelname, "Distribution Plot "), 
                             y = "Density", x = paste(modelname,"Prob")) +
          theme_bw()
}
```

Function that calculate performance.
```{r}
performanceMeasures <- function(pred, true, model.name = "model",threshold) {
  dev.norm <- -2 * logLikelihood(true, pred)/length(pred) 
  cmat <- table(actual = true, predicted = (pred > threshold))
  accuracy <- sum(diag(cmat)) / sum(cmat)
  precision <- cmat[2, 2] / sum(cmat[, 2])
  recall <- cmat[2, 2] / sum(cmat[2, ])
  f1 <- 2 * precision * recall / (precision + recall)
  list(cmat,data.frame(model = model.name, precision = precision,recall = recall, f1 = f1, accuracy = accuracy))
}
```

Function that print the graphs and table.
```{r}
performance <- function(model,dCal,cal_true,dTrain,train_true,predValue=TRUE,threshold=0.5,modelname){
  
  if (predValue){
    pred_value <- predict(model, newdata = dCal)[,pos]
    pred_train <- predict(model, newdata = dTrain)[,pos]
  }else {
    pred_value <- predict(model, newdata=dCal, type="response")
    pred_train <- predict(model, newdata=dTrain)
  }
  
  predObj <- ROCR::prediction(pred_value, cal_true)
  precObj <- ROCR::performance(predObj, "prec")
  recObj  <- ROCR::performance(predObj, "rec")
  perf    <- ROCR::performance(predObj, "tpr", "fpr")

  pf <- data.frame(FalsePositiveRate=perf@x.values[[1]],
                    TruePositiveRate=perf@y.values[[1]])
  
  print(plotROC(pf,modelname))
  
  print(distribution(pred_value, as.factor(cal_true),modelname))
  
  pnull      <- mean(cal_true==pos)
  callog     <- logLikelihood(cal_true, pred_value)
  nulllog    <- logLikelihood(cal_true, pnull)
  trainAUC   <- calcAUC(pred_train, train_true)
  calAUC     <- calcAUC(pred_value, cal_true)
  
  modelm <- matrix(c(nulllog,callog,trainAUC,calAUC), byrow=TRUE, nrow = 1)
  colnames(modelm) <- c("null_log", "cal_log", "trainAUC","calAUC")
  print(modelm)
  
  trainperf_df <- performanceMeasures(train_true==pos, pred_train >= threshold, model.name="training",threshold)
  testperf_df <- performanceMeasures(cal_true==pos, pred_value >= threshold, model.name="calibration",threshold)
  train_df   <- trainperf_df[[2]]
  test_df <- testperf_df[[2]]
  perftable   <- rbind(train_df,test_df) 
  print(knitr::kable(testperf_df[[1]]))
  print(perftable)
  df <- data.frame(modelname,trainAUC,calAUC,callog)
  names(df) <- c("modelname","trainAUC", "calAUC", "logLikelihood")
  alldf <- cbind(df,test_df[,c(2,3,4,5)])
  modelMeasure <<- rbind(modelMeasure, alldf)
}
```

#### Decision Tree with all variables
```{r}
formula <- paste(outcome,' ~ ', paste(vars, collapse=" + "), sep='')
decision_tree_all <- rpart(formula, data=dTrain)
```

```{r}
performance(decision_tree_all,dCal,dCal[,outcome],dTrain,dTrain[,outcome],predValue=TRUE, modelname="Decision Tree (all)")
```

This decision tree model that containing all variables selected including the indicator variables with a log likelihood of -7631.1 which is better than null model (-11762.61). The AUC score for training and calibration dataset doesn't vary a lot, the calibration AUC is 0.824 which is slightly better than train AUC, 0.822. Same for the precision, recall, fl and accuracy, those scores are very close between the training and calibration score. The ROC curve doesn't look very good (since the AUC is only 0.824) and we can see from the distribution plot that there is overlap between the positive(yes) and negative(no) for the probability that generated especially when x axis is around 0.3. Different threshold (0.5-0.9) were tested, but not much difference, so we'll just stick with 0.5 for this model. High accuracy is what we are aiming for, both false positive/false negative will need to some potential problems, for example cost for countermeasures and employees dissatisfaction. However, the accuracy for this model doesn't look very impressive and improvement is needed.

#### Decision Tree without indicators
```{r}
formula <- paste(outcome,' ~ ', paste(c(catVars,numericVars), collapse=' + ', sep=''))
decision_tree_wNind <- rpart(formula, data=dTrain)
```

```{r}
performance(decision_tree_wNind,dCal,dCal[,outcome],dTrain,dTrain[,outcome],predValue=TRUE, modelname="Decision Tree (no_ind)")
```

This model excluded all the indicator variables and there is no differences between the model above. AUC, log likelihood, precision, recall, f1 and accuracy, everything is the same suggesting the indicator variables dont affect the performance of the decision tree model. 

#### Decision Tree with the selected variables
```{r}
formula <- paste(outcome,' ~ ', paste(selVars, collapse=' + '), sep='')
decision_tree_sel <- rpart(formula, data=dTrain)
```

```{r}
performance(decision_tree_sel,dCal,dCal[,outcome],dTrain,dTrain[,outcome],predValue=TRUE, modelname="Decision Tree (sel)")
```

Surprisingly,the performance of decision tree with selected variables is exactly the same as the two models above. Again, this suggests that the numeric variables of the dataset which are not good predictors with no influences on the model.  

#### Logistic Regression with all variables
```{r message=FALSE, warning=FALSE}
formula <- paste(outcome, paste(c(catVars,numericVars), collapse=" + "), sep=" ~ ")
logr_all <- glm(formula=formula, data=dTrain, family=binomial(link="logit"))
```

```{r message=FALSE, warning=FALSE}
performance(logr_all,dCal,dCal[,outcome],dTrain,dTrain[,outcome],predValue=FALSE, threshold= 0.57, modelname="LogisticR (all)")
```

This logistic regression model that containing all variables with a log likelihood of -6863.677 which is better than null model (-11762.61) and decision tree models (-7631.1). The AUC score for training and calibration dataset are 0.8899027, 0.8881454 respectively. Again, the AUC which is greater than decision tree models. The ROC curve looks better than the decision tree model as well and we can see from the distribution plot that there is more obvious cutoff between yes and no values and this is the threshold value that we picked for analysis. Also, the precision, recall, f1 and accuracy scores for calibration set which are slightly better than decision tree models.

```{r message=FALSE, warning=FALSE}
formula <- paste(outcome, paste(selVars, collapse=" + "), sep=" ~ ")
logr_sel <- glm(formula=formula, data=dTrain, family=binomial(link="logit"))
```

```{r message=FALSE, warning=FALSE}
performance(logr_sel,dCal,dCal[,outcome],dTrain,dTrain[,outcome],predValue=FALSE,threshold= 0.57, modelname="LogisticR (sel)")
```

Both the likelihood and AUC are slightly worse than the logistic regression with all variables, there is not much differences in other performance measures this further suggest the numeric variables that not not predictors. 

#### XGBoost
```{r}
tplan <- vtreat::designTreatmentsZ(dTrain, varlist, 
                                   minFraction= 0.025,
                                   verbose=FALSE)
sf <- tplan$scoreFrame
newvars <- sf$varName[sf$code %in% c("lev", "clean", "isBAD")]

trainVtreat <- as.matrix(vtreat::prepare(tplan, dTrain, varRestriction = newvars))
calVtreat <- as.matrix(vtreat::prepare(tplan, dCal, varRestriction = newvars))
testVtreat <- as.matrix(vtreat::prepare(tplan, dTest, varRestriction = newvars))
```

```{r}
cv <- xgb.cv(trainVtreat, 
             label = dTrain[,outcome]==pos,
             params=list(objective="binary:logistic"),
             nfold=5,
             nrounds=100,
             print_every_n=10,
             metrics="logloss")

evalframe <- as.data.frame(cv$evaluation_log)
NROUNDS <- which.min(evalframe$test_logloss_mean)
```

```{r}
xgboost_model <- xgboost(data=trainVtreat, 
                 label=dTrain[,outcome]==pos,
                 params=list(objective="binary:logistic"),
                 nrounds=NROUNDS,
                 verbose=FALSE)
```

```{r}
performance(xgboost_model,calVtreat,dCal[,outcome],trainVtreat,dTrain[,outcome],predValue=FALSE, modelname="xgboost")
```

This is the best classification model that we've built so far with AUC greater than 0.99 and log likelihood of -1675.306. The precision, recall, f1 and accuracy are 0.985,0.961,0.972 and 0.967, respectively. This is far better than the decision tree and logistic models.  

#### Put all models together 
```{r}
df <- melt(modelMeasure[,names(modelMeasure) != "logLikelihood"],id.vars = "modelname" ,variable.name = 'measure')

ggplot(df, aes(measure,value,group=modelname,colour=modelname)) + 
  geom_point(size=1)+
  geom_line()
```

Based on the performance measure, it is very clear that xgboost outperformed decision tree and logistic regression model. 

### Evaluate xgboost model on the test set

















